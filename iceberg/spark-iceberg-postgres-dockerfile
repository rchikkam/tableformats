FROM apache/spark:3.5.6

ENV ICEBERG_VERSION=1.5.2
ENV AWS_REGION=us-east-1

# Set working directory to Spark's jars folder
WORKDIR /opt/spark/jars

# Download necessary JARs - MUST include AWS SDK v2, not v1!
RUN curl -LO https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.6/hadoop-aws-3.3.6.jar && \
    curl -LO https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/3.3.6/hadoop-common-3.3.6.jar && \
    curl -LO https://repo1.maven.org/maven2/org/postgresql/postgresql/42.7.3/postgresql-42.7.3.jar && \
    curl -LO https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/${ICEBERG_VERSION}/iceberg-spark-runtime-3.5_2.12-${ICEBERG_VERSION}.jar && \
    curl -LO https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-aws-bundle/${ICEBERG_VERSION}/iceberg-aws-bundle-${ICEBERG_VERSION}.jar && \
    curl -LO https://repo1.maven.org/maven2/software/amazon/awssdk/bundle/2.20.18/bundle-2.20.18.jar && \
    curl -LO https://repo1.maven.org/maven2/software/amazon/awssdk/url-connection-client/2.20.18/url-connection-client-2.20.18.jar

WORKDIR /opt/spark

# Set Spark SQL as default command with Iceberg + JDBC + S3 configs
ENTRYPOINT ["/opt/spark/bin/spark-sql",\
  "--conf", "spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions",\
  "--conf", "spark.sql.catalog.rest_backend=org.apache.iceberg.spark.SparkCatalog",\
  "--conf", "spark.sql.catalog.rest_backend.type=jdbc",\
  "--conf", "spark.sql.catalog.rest_backend.io-impl=org.apache.iceberg.aws.s3.S3FileIO",\
  "--conf", "spark.sql.catalog.rest_backend.uri=jdbc:postgresql://catalog-db:5432/iceberg",\
  "--conf", "spark.sql.catalog.rest_backend.jdbc.user=admin",\
  "--conf", "spark.sql.catalog.rest_backend.jdbc.password=password",\
  "--conf", "spark.sql.catalog.rest_backend.warehouse=s3a://warehouse0/",\
  "--conf", "spark.sql.catalog.rest_backend.s3.endpoint=http://minio:9000",\
  "--conf", "spark.sql.catalog.rest_backend.s3.access-key-id=admin",\
  "--conf", "spark.sql.catalog.rest_backend.s3.secret-access-key=password",\
  "--conf", "spark.sql.catalog.rest_backend.s3.path-style-access=true",\
  "--conf", "spark.sql.catalog.rest_backend.s3.region=us-east-1",\
  "--conf", "spark.sql.defaultCatalog=rest_backend",\
  "--conf", "spark.hadoop.fs.s3a.endpoint=http://minio:9000",\
  "--conf", "spark.hadoop.fs.s3a.access.key=admin",\
  "--conf", "spark.hadoop.fs.s3a.secret.key=password",\
  "--conf", "spark.hadoop.fs.s3a.connection.ssl.enabled=false",\
  "--conf", "spark.hadoop.fs.s3a.path.style.access=true",\
  "--conf", "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem",\
  "--conf", "spark.hadoop.aws.region=us-east-1",\
  "--conf", "spark.driver.extraJavaOptions=-Daws.region=us-east-1",\
  "--conf", "spark.executor.extraJavaOptions=-Daws.region=us-east-1"]

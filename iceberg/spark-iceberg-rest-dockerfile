FROM apache/spark:3.5.6

# Version configuration
ENV ICEBERG_VERSION=1.5.2
ENV HADOOP_VERSION=3.3.6
ENV AWS_SDK_VERSION=2.20.18
ENV AWS_REGION=us-east-1
ENV SPARK_SCALA_VERSION=3.5_2.12

# Set working directory to Spark's jars folder
WORKDIR /opt/spark/jars

# Download necessary JARs with consistent versions
RUN curl -LO https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${HADOOP_VERSION}/hadoop-aws-${HADOOP_VERSION}.jar && \
    curl -LO https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-${SPARK_SCALA_VERSION}/${ICEBERG_VERSION}/iceberg-spark-runtime-${SPARK_SCALA_VERSION}-${ICEBERG_VERSION}.jar && \
    curl -LO https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-aws-bundle/${ICEBERG_VERSION}/iceberg-aws-bundle-${ICEBERG_VERSION}.jar && \
    curl -LO https://repo1.maven.org/maven2/software/amazon/awssdk/bundle/${AWS_SDK_VERSION}/bundle-${AWS_SDK_VERSION}.jar && \
    curl -LO https://repo1.maven.org/maven2/software/amazon/awssdk/url-connection-client/${AWS_SDK_VERSION}/url-connection-client-${AWS_SDK_VERSION}.jar

WORKDIR /opt/spark

# Set Spark SQL as entrypoint with complete configuration including region
ENTRYPOINT ["/opt/spark/bin/spark-sql", \
  "--conf", "spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions", \
  "--conf", "spark.sql.catalog.rest_backend=org.apache.iceberg.spark.SparkCatalog", \
  "--conf", "spark.sql.catalog.rest_backend.type=rest", \
  "--conf", "spark.sql.catalog.rest_backend.uri=http://rest-catalog:8181", \
  "--conf", "spark.sql.catalog.rest_backend.warehouse=s3a://warehouse0/", \
  "--conf", "spark.sql.catalog.rest_backend.io-impl=org.apache.iceberg.aws.s3.S3FileIO", \
  "--conf", "spark.sql.catalog.rest_backend.s3.endpoint=http://minio:9000", \
  "--conf", "spark.sql.catalog.rest_backend.s3.access-key-id=admin", \
  "--conf", "spark.sql.catalog.rest_backend.s3.secret-access-key=password", \
  "--conf", "spark.sql.catalog.rest_backend.s3.path-style-access=true", \
  "--conf", "spark.sql.catalog.rest_backend.s3.region=us-east-1", \
  "--conf", "spark.sql.catalog.rest_backend.client.region=us-east-1", \
  "--conf", "spark.sql.defaultCatalog=rest_backend", \
  "--conf", "spark.hadoop.fs.s3a.endpoint=http://minio:9000", \
  "--conf", "spark.hadoop.fs.s3a.access.key=admin", \
  "--conf", "spark.hadoop.fs.s3a.secret.key=password", \
  "--conf", "spark.hadoop.fs.s3a.path.style.access=true", \
  "--conf", "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem", \
  "--conf", "spark.hadoop.fs.s3a.connection.ssl.enabled=false", \
  "--conf", "spark.hadoop.aws.region=us-east-1", \
  "--conf", "spark.driver.extraJavaOptions=-Daws.region=us-east-1", \
  "--conf", "spark.executor.extraJavaOptions=-Daws.region=us-east-1"]
